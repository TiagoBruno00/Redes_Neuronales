# üß† Autoencoders y Clasificadores Convolucionales sobre Fashion-MNIST

An√°lisis experimental de arquitecturas de Redes Neuronales y la **Optimizaci√≥n de Hiperpar√°metros** para tareas de reconstrucci√≥n y clasificaci√≥n de im√°genes.

---
[Aqu√≠ puedes insertar una imagen como la Figura 1 (esquema del autoencoder) para hacerlo visualmente atractivo. Si la subes a GitHub, el enlace ser√° local.]

## üìù Resumen del Proyecto

Este proyecto implementa y compara distintos tipos de **Autoencoders** (Lineal y Convolucionales) y **Clasificadores** sobre el conjunto de datos **Fashion-MNIST**. El objetivo principal es reflexionar sobre la elecci√≥n de **hiperpar√°metros** (√©pocas, tama√±o de lote, tasa de aprendizaje, dropout) y su impacto en el rendimiento.

### üéØ Conclusiones Clave

* **Superioridad Convolucional:** Las Redes Convolucionales demostraron ser **superiores** a las *feed-forward* (Lineales) para este conjunto de datos, tanto para comprimir y reconstruir im√°genes como para clasificarlas.
* **Reconstrucci√≥n (Autoencoders):** Aumentar el tama√±o de la capa oculta mejor√≥ la reconstrucci√≥n. El modelo **Raschka (n=128)** alcanz√≥ un error comparable al Lineal (n=512) pero en solo **14 √©pocas** (vs. 60 √©pocas).
* **Optimizaci√≥n por Arquitectura:** La elecci√≥n de hiperpar√°metros depende del tipo de red:
    * **Redes Convolucionales** (Consigna, Raschka) aprendieron mejor con **lotes peque√±os (32)** y tasas de aprendizaje **bajas (0.0005)**.
    * **Redes Lineales** (*Feed-Forward*) funcionaron mejor con **lotes grandes (1000)** y tasas de aprendizaje **altas (0.001)**.
* **Estrategias de Regularizaci√≥n:** El *Dropout* fue m√°s efectivo en la red Lineal, mientras que el *Pooling* ayud√≥ a la generalizaci√≥n en las convolucionales.
* **Clasificaci√≥n:** Se aplic√≥ **Feature Extraction** (Transfer Learning) sobre el clasificador "Consigna" para mitigar la dependencia de valores iniciales, obteniendo mejores resultados.

## üõ†Ô∏è Modelos Implementados

| Modelo | Tipo | Capa Oculta | √âpocas | Tasa de Aprendizaje | Dropout |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Autoencoder Lineal** | Feed-Forward | 512 | 60 | 0.001 | 0.1 |
| **Autoencoder Raschka** | Convolucional | 128 | 14 | 0.0005 | 0 |
| **Clasificador Consigna FE** | Convolucional | 128 | 26 | 0.0005 | 0.8 |

## üìÅ Estructura del Repositorio

Aseg√∫rate de que tu repositorio contenga los siguientes archivos para que el proyecto est√© completo:

* `Tiago_Bruno_tfi_2023.pdf` (Tu informe de investigaci√≥n)
* `autoencoder_lineal.py` (O el nombre del archivo con tu autoencoder lineal)
* `autoencoder_consigna.py` (O el nombre del archivo con tu autoencoder convolucional 'consigna')
* `autoencoder_raschka.py` (O el nombre del archivo con tu autoencoder convolucional 'raschka')
* `clasificador.py` (O el archivo que contiene el c√≥digo de tus clasificadores)
* `requirements.txt` (Una lista de las librer√≠as necesarias: `torch`, `numpy`, `matplotlib`, etc.)

---
## üöÄ C√≥mo Ejecutar el C√≥digo (Pendiente de tu c√≥digo)

[Una vez que subas los archivos de c√≥digo, puedes indicar los pasos exactos aqu√≠, por ejemplo:]
1. Clonar el repositorio: `git clone https://github.com/TiagoBruno00/Redes_Neuronales.git`
2. Instalar dependencias: `pip install -r requirements.txt`
3. Ejecutar el entrenamiento del clasificador: `python clasificador.py`
